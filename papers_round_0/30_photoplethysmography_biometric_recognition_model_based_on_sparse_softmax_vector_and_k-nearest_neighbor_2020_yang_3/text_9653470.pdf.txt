Research Article
Photoplethysmography Biometric Recognition Model Based on
Sparse Softmax Vector and k-Nearest Neighbor
Junfeng Yang,1 Yuwen Huang,1 Fuxian Huang,1 and Gongping Yang
2
1School of Computer, Heze University, Heze 274015, China
2School of Software, Shandong University, Jinan 250101, China
Correspondence should be addressed to Gongping Yang; gpyang@sdu.edu.cn
Received 7 August 2020; Revised 19 September 2020; Accepted 10 October 2020; Published 23 October 2020
Academic Editor: Cesare F. Valenti
Copyright © 2020 Junfeng Yang et al. Tis is an open access article distributed under the Creative Commons Attribution License,
which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
Photoplethysmography (PPG) biometric recognition has recently received considerable attention and is considered to be a
promising biometric trait. Although some promising results on PPG biometric recognition have been reported, challenges in noise
sensitivity and poor robustness remain. To address these issues, a PPG biometric recognition framework is presented in this
article, that is, a PPG biometric recognition model based on a sparse softmax vector and k-nearest neighbor. First, raw PPG data
are rerepresented by sliding window scanning. Second, three-layer features are extracted, and the features of each layer are
represented by a sparse softmax vector. In the first layer, the features are extracted by PPG data as a whole. In the second layer, all
the PPG data are divided into four subregions, then four subfeatures are generated by extracting features from the four subregions,
and finally, the four subfeatures are averaged as the second layer features. In the third layer, all the PPG data are divided into 16
subregions, then 16 subfeatures are generated by extracting features from the 16 subregions, and finally, the 16 subfeatures are
averaged as the third layer features. Finally, the features with first, second, and third layers are combined into three-layer features.
Extensive experiments were conducted on three PPG datasets, and it was found that the proposed method can achieve a
recognition rate of 99.95%, 97.21%, and 99.92% on the respective sets. Te results demonstrate that the proposed method can
outperform current state-of-the-art methods in terms of accuracy.
1. Introduction
As a kind of biological signal, photoplethysmography (PPG) is
difficult to steal or replicate, which has advantages of inherent
antispoofing and liveness detection, and it can be conveniently
recorded with just a combination of light-emitting diodes and
photodiodes (PDs) from any part of the body and is thus very
cost-effective compared to other biometric traits. Gu et al. [1]
were the first group to investigate PPG for user authentication,
considering four-feature parameters and achieving 94% ac-
curacy. Since then, PPG biometric recognition has attracted
increasing research interest and is regarded as one of the most
promising biometric techniques. For PPG biometric recogni-
tion, many fiducial-point and nonfiducial-point approaches
have been proposed in the past.
In
fiducial-point-based
approaches,
features
are
extracted from systolic peaks, diastolic peaks, dicrotic
notches, interpulse intervals, amplitudes of peaks, etc. Given
the variability in PPG shape in different states, fiducial
detection on raw PPG signals might be unsuccessful or
incorrect. Terefore, many researchers [2–5] extended the
paradigm to first derivatives (FDs) or second derivatives
(SDs) of raw PPG signals and used similar points on FDs and
SDs as features for recognition. For example, Kavsao˘glu et al.
[2] proposed that a feature with 40 dimensions was extracted
from raw PPG and its derivatives for 30 healthy subjects, and
k-nearest neighbor (k-NN) was used for classification.
Chakraborty and Pal [3] proposed that features with 12
dimensions were extracted from filtered PPG and its de-
rivatives, and linear discriminant analysis (LDA) was used
for classification, achieving 100% accuracy for 15 subjects.
Kavsao˘glu et al. [4] showed that features with 20 dimensions
were extracted from a PPG signal and its second derivative,
which achieved a 95% recognition rate by using 10-fold
Hindawi
Journal of Electrical and Computer Engineering
Volume 2020, Article ID 9653470, 9 pages
https://doi.org/10.1155/2020/9653470
cross-validation. Al-Sidani et al. [5] proposed that a feature
with 40 dimensions was extracted from filtered PPG and its
derivatives, and then the k-NN classifier was applied,
achieving a 100% recognition rate for 23 subjects. Jindal et al.
[6] presented a novel two-stage technique for PPG biometric
identification involving the clustering of individual PPG
sources into different groups and using deep belief networks
as classification models; the approach was tested on the
TROIKA dataset and achieved an accuracy of 96.1%. Nadzri
et al. [7] applied a low-pass filter to remove unwanted noise
from the PPG signal, and then discriminant features in-
cluding systolic peaks, diastolic peaks, and dicrotic notches
were extracted from the filtered PPG signals. Later, a Bayes
network (BN), na¨ıve Bayes (NB), radial basis function (RBF),
and multilayer perceptron (MLP) were used to classify the 21
subjects using the discriminant features. Sancho et al. [8]
studied several feature extractors (e.g., cycles average,
multicycle based on the time domain, and the Karhunen-
Lo`eve transform average) and matching metrics (Manhattan
and Euclidean distances) that had been tested by using four
different PPG databases (CapnoBase, MIMIC-II, Berry, and
Nonin), and an optimal equal error rate (EER) of 1.0% was
achieved for CapnoBase.
Many of the previous methods focused on fiducial ap-
proaches, but fiducial-point detection in any condition is
error prone. Terefore, many researchers [9–13] have
researched
nonfiducial-point-based
approaches.
Non-
fiducial approaches take a more holistic approach in which
features are extracted statistically based on the overall signal
morphology [10]. Spachos et al. [9] proposed a PPG bio-
metric recognition method based on LDA/k-NN, and an
EER of 0.5% was achieved for 14 subjects of the OpenSignal
dataset. Karimian et al. [10] used an anon-fiducial approach
for PPG with a discrete wavelet transform (DWT) and k-NN
and reported a 99.84% accuracy rate with an EER of 1.31%
on the CapnoBase dataset. Yadav et al. [11] proposed a
method for PPG authentication based on a continuous
wavelet transform (CWT) and direct linear discriminant
analysis (DLDA) and reported a 0.46% EER on the Cap-
noBase dataset. Farago et al. [12] presented the correlation-
based
nonfiducial
features
extraction
technique
and
achieved a 98% accuracy rate for 30 subjects by computing
correlations of the individual’s peak-to-peak interval to a
reference PPG peak-to-peak interval. Lee et al. [13] tried to
use a discrete cosine transform (DCT) for extracting features
from the preprocessed PPG data, and the extracted features
were used as the input variables for machine-learning
techniques, e.g., decision tree, k-NN, and random forest
(RF); the accuracy of these aforementioned algorithms was
93%, 98%, and 99%, respectively.
Recently, deep-learning methodologies have been used
in PPG biometric recognition. Everson et al. [14] formulated
a completely personalized data-driven approach and used a
four-layer deep neural network employing two convolu-
tional neural network (CNN) layers in conjunction with two
long short-term memory (LSTM) layers, followed by a dense
output layer for modeling the temporal sequence inherent
within the pulsatile signal representative of cardiac activity.
Te proposed network was evaluated on the TROIKA
dataset, which was collected from 12 subjects involved with
physical activities and achieved an average accuracy of 96%.
Biswas et al. [15] presented a novel deep-learning framework
(CorNET) to efficiently estimate heart rate (HR) information
and performed biometric identification using only wrist-
worn, single-channel PPG signals collected in an ambulant
environment, and an average accuracy of 96% was achieved
for 20 subjects on the TROIKA dataset. Hwang and Hat-
zinakos [16] proposed a novel deep-learning-based verifi-
cation model using PPG signals, built a personalized data-
driven network by employing a CNN with LSTM, and
modeled the time-series sequence inherent within the PPG
signal.
From the current studies for PPG biometric recognition,
the nonfiducial-based approach is better than the fiducial-
based approach with the same conditions, and the deep-
learning method is initially launched. However, there are
several shortcomings in these studies: (1) most of the PPG
biometric methods are sensitive to noise and have poor
robustness, and (2) deep learning has good recognition
performance, but it is not easy to train on small-scale data by
means of data augmentation. In addition, deep learning
requires powerful computational resources, and it has too
many hyperparameters that need intricate adjustment. In-
spired by [17–21], a PPG biometric recognition model based
on a sparse softmax vector (SSV) and k-NN is proposed
herein, and the main contributions of our work are the
following.
(1) Raw PPG data are rerepresented by sliding window
scanning. Raw PPG data is rerepresented by a sliding
window so as to maximize the total data and reduce
the impact caused by too little data
(2) A three-layer feature-extraction method based on a
sparse softmax vector is presented. Most of the
sparse representation methods are based on all the
raw data, which ignores the local data. However, the
local data tend to show more detailed and dis-
criminative information than all the data. Hence, all
the data and their subregions are combined to obtain
softmax vectors by using three layers
(3) To verify the recognition performance of k-NN, a
variety of classifiers are also used for experiments,
such as RF, a linear discriminating classifier (LDC),
and NB
Te rest of this article is organized as follows. In Section
2, the proposed method is explained. In Section 3, the ex-
perimental process is detailed and the results obtained are
presented. Finally, in Section 4, conclusions and directions
for future work are presented.
2. Proposed Method
To perform PPG biometric recognition, a PPG biometric
recognition framework was designed. First (mentioned in
Section 2.1), raw PPG data are rerepresented by sliding
window scanning. Second (mentioned in Section 2.2), the
final
discriminative
features
are
generated
from
the
2
Journal of Electrical and Computer Engineering
rerepresented PPG data. Finally (mentioned in Section 2.3),
the classification procedure is performed for the final fea-
tures. Te flow diagram of PPG biometric recognition using
the proposed framework is shown in Figure 1. In the fol-
lowing subsections, preprocessing of raw PPG data, base
feature extraction, and three-layer feature extraction and
classification will be detailed.
2.1. Preprocessing Raw PPG Data. Preprocessing means that
raw PPG data are rerepresented by sliding window scanning
in our approach, and the input raw PPG data of a subject can
be segmented into N-dimensional segments. Suppose that
one has a sliding window of dimension M, an M-dimen-
sional vector will be generated by scanning the raw PPG data
for one step, and N − M + 1 vectors are produced by sliding
for N − M + 1 steps; finally, these vectors are concatenated to
a (M, L) matrix, and the matrix is denoted by Aml. Tat is to
say, the raw PPG data of K subjects are rerepresented as Aml.
L can be computed as follows:
L � 􏽘
K
k�1
Nk − M + 1
�
􏼁,
(1)
where Nk is the number of class subjects. Te flow diagram of
raw PPG data rerepresented using sliding window scanning
is shown in Figure 2.
2.2. Features Extraction
2.2.1. Basic Acquisition of New Feature. In this section, we
use the alternating direction method of multipliers
(ADMM) [19, 22] to solve the sparse representation
problem. Te sparse representation method is more ro-
bust than others for biometric recognition [17, 23], so it is
used in the getting new feature (GNF) process to obtain
softmax vectors.
Suppose that one has K-class subjects, y represents a
testing
sample,
and
X � [X1, X2, . . . , XK]
represents
training samples. In terms of a sparse-representation-based
classifier (SRC) [17, 24] and dictionary learning [19], the
representation model can be transformed into the following
minimization problem:
min
w ‖y − Xw‖2
2 + λ‖w‖1,
(2)
where λ is a scalar constant, and ‖ · ‖1 and ‖ · ‖2 represent the
L1-norm and L2-norm, respectively. Sparse-representation-
based classifier codes a testing sample by a sparse linear
combination of all training samples and classifies it to the
class which has minimum representation error. Terefore,
we propose to use the representation error vector of all
classes to represent a sample. After solving the represen-
tation coefficients w, the representation error of each class
can be computed as follows:
rk � y − Xkwk
����
����
2
2,
k � 1, 2, 3, . . . , K,
(3)
where Xk is the sample set with respect to class k and wk the
coefficient vector associated with class k. Ten, the softmax
vector Sv ∈ RK can be computed by a softmax function as
follows:
Sv �
e−r1
􏽐K
k�1 e−rk,
e−r2
􏽐K
k�1 e−rk,
e−r3
􏽐K
k�1 e−rk, . . . ,
e−rK
􏽐K
k�1 e−rk
􏼢
􏼣
T
,
(4)
where (4) can be written as Sv � [c1, c2, c3, . . . , cK]T. If the
testing sample y belonged to the ith class (i ≤ K), ci should be
larger than other atoms in the softmax vector Sv, which is
called class discrimination. Te above process of obtaining
softmax vector c is named as getting a new feature on
dictionary X (GNFX). For convenience, the entire procedure
of computing the softmax vector Sv for a given testing
sample y is defined as
Sv � GNFX(y, w, K),
(5)
and the solution process of the sparse representation co-
efficient w can be seen in [20, 21]. Te detailed procedure for
extracting the sparse softmax vector of a test sample is
summarized in Algorithm 1.
2.2.2. Tree-Layer Feature Extraction. Inspired by the three-
layer spatial pyramid model in [20, 21], a three-layer feature-
extraction method is proposed herein. First, Aml is divided
into X and Y, where X represents the training sample
matrix and Y a testing sample matrix; the class number is K.
Second, the final features are generated by the concate-
nation of three layers. For the first layer, each sample of X
and Y is taken as a whole to call GNFX, and then a K-
dimensional sparse softmax vector Sv1 is generated. For the
second layer, each sample of X and Y is divided into four
subregions,
and
each
subregion
is
taken
to
call
GNFXi(i � 1, 2, 3, 4), the results of the operation are av-
eraged, and a K-dimensional sparse softmax vector Sv2 is
generated. For the third layer, each sample of X and Y may
be divided into 16 subregions, and a K-dimensional sparse
softmax vector Sv3 is generated similarly. Finally, the sparse
softmax vectors generated by X and Y are integrated into a
training template and a new testing sample matrix, re-
spectively. Figure 3 shows the process by which a sample y
is divided into four subregions and is generated by a sparse
softmax vector. Figure 4 shows the process by which the
sparse softmax matrix is generated by three-layer feature
extraction for y.
2.2.3. Algorithm of Tree-Layer Feature Extraction. Te
computing process of a three-layer sparse softmax vector is
shown in detail in Algorithm 2.
2.3. Classification. Machine-learning algorithms used in
biometric recognition are used to try and solve two different
problems. Biometric identification is seen as a multiclass
classification problem. Biometric verification is seen as a
one-class classification problem. Although there are some
algorithms specifically used for each problem, some mul-
ticlass classification problems can be adapted to solve one-
class classification problems. In general, the matching uses
Journal of Electrical and Computer Engineering
3
similarity and machine-learning techniques to produce its
decision [14], and in this study, machine-learning tech-
nology is used to make decisions. Here, the training template
and test sample generated by the proposed feature-extrac-
tion method are input into the machine-learning classifier,
and then the recognition rate is output. Te classifiers used
here include k-NN [4, 10, 25], NB [7, 26], RF [13, 27], and
LDC [3].
3. Experiments and Results
3.1. PPG DataSets. Tree public datasets are introduced
here: Beth Israel Deaconess Medical Center (BIDMC),
Multiparameter Intelligent Monitoring for Intensive Care
(MIMIC), and CapnoBase. Table 1 summarizes the char-
acteristics of these datasets, with a subsection devoted to a
specific description of each dataset.
Training
set
Raw 
PPG data
Testing
set
Preprocessing
the raw PPG data
Extracting feature
based on SSV
Extracting feature
based on SSV
Three-layer
feature extraction
Training
template
Classifier
Decision
Testing
sample
Figure 1: Flow diagram of the proposed photoplethysmography (PPG) biometric recognition framework.
Sequence data
…
M-dim
M-dim
M-dim
Raw PPG data of K subjects
Concatenate
M-dim
M-dim
M-dim
M-dim
N1 – M + 1-dim
N1 – M + 1
Instances
N2 – M + 1
Instances
Nk – M + 1
Instances
N2 – M + 1-dim
Nk – M + 1-dim
L-dim
K-class
Sliding
Sliding
Sliding
N1-dim
N2-dim
Nk-dim
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
Figure 2: Illustration of raw photoplethysmography (PPG) data rerepresented using sliding window scanning.
Input: K-class subjects, training sample X and a testing sample y normalized with L2-norm, parameters α � 10− 4, μ � 10− 1,
λ � 10− 4, identity matrix I
Output: softmax vector Sv of K-class
(1)
initialize w0 � 0, z0 � 0, Λ0 � 0, i � 0
(2)
repeat
(3)
update w: wi+1 � (XTX + μI)− 1(XTy − Λi + μzi)
(4)
update z: zi+1 � shrinkageλ/μ(wi+1 + (Λi/μ))
(5)
update Λ: Λi+1 � Λi + μ(wi+1 − zi+1)
(6)
until convergence
(7)
let r � ⌈⌉
(8)
for each k in {1, 2, 3, . . . , K} do
(9)
let rk � ‖y − XkWk‖2
2
(10)
let r � [r, e− rk]
(11)
end for
(12)
let Sv � r/sum(r)
(13)
output Sv ∈ RK×1
ALGORITHM 1: Extracting sparse softmax vector of a testing sample.
4
Journal of Electrical and Computer Engineering
3.1.1. BIDMC Dataset. Te BIDMC dataset [28, 29] is a
dataset of electrocardiogram (ECG), pulse oximetry (PPG),
and impedance pneumography respiratory signals acquired
from the intensive care unit (ICU) patients. Te dataset
comprises 53 8-minute recordings of ECG, PPG, and im-
pedance
pneumography
signals
(sampling
frequency,
fs � 125 Hz) acquired from adult patients (aged 19–90+, 32
females). Te patients in the dataset were randomly selected
from a larger cohort that were admitted to medical and
surgical intensive care units at the BIDMC, Boston, Mass.,
USA.
3.1.2. MIMIC Database. Te MIMIC database [30, 31]
collects recordings of PLETH, ABP, RESP, etc. of patients in
ICUs and is published on PhysioBank ATM for free. Tese
recordings have multiple data formats. Te partial record-
ings of 32 patients were downloaded for this work, that is,
Nos. 039, 055, 210, 211, 212, 216, 218, 219, 221, 224, 225, 230,
240, 253, 276, 281, 284, 403, 408, 410, 411, 437, 439, 444, 449,
451, 466, 471, 472, 474, 476, and 484. Te recordings of each
patient include .hea, .mat, .info, and plotATM.m files (.mat is
the matrix of raw signal value, .info the signal name, and
other information about .mat; .hea is needed to read .mat
files using applications in the WFDB Software Package or
functions in the WFDB Toolbox for MATLAB (MathWorks,
USA)). plotATM.m is a function that reads .mat and .info
files and plots the converted data. PLETH is the PPG data
signal needed in this work and its frequency is 125 Hz.
3.1.3. CapnoBase Dataset. Te CapnoBase dataset [32, 33]
contains raw PPG signals for 42 cases of 8-minute duration,
pulse peak and artifact labels validated by an expert rater,
reference CO2 signal and derived instantaneous respiratory
rate for all cases, reference electrocardiogram (ECG) signal
with R peak and artifact labels validated by an expert rater,
and reference instantaneous heart rate derived from ECG
and PPG pulse peak labels. References [4, 5] also used the
dataset for PPG biometric recognition and achieved good
results.
3.2. Experiments. To evaluate the performance of PPG
biometric recognition, extensive experiments were per-
formed under the MatLab2018b programming environment.
To avoid special cases, all experiments were run multiple
times, and the average recognition rates are reported. Fol-
lowing
experimental
settings
in
the
experiments,
experimental data of each group are randomly obtained
from a continuous sequence of raw PPG data, 80% of which
are used for training and 20% for testing.
3.2.1. Evaluation Metrics. To evaluate the performance of
the proposed method, experiments were conducted using
the following method. For the identification problem, the
recognition rate is used as the evaluation criterion, which is
the percentage of correctly recognized testing samples, de-
fined as follows:
Recognition rate � Ncorrect
Ntesting
,
(6)
where Ntesting is the total number of probe samples and
Ncorrect the number of probe samples that are correctly
identified.
3.2.2. Performance of Proposed Method. Experiments were
conducted on the datasets, that is, BIDMC, MIMIC, and
CapnoBase, to examine the performance of the proposed
approach. All subjects for each dataset participated in the
experiments. For a single subject in each dataset, PPG data
points of a cycle were randomly selected to participate in the
experiments, 80% of which were used for training and 20%
for testing. Average recognition rates were summarized
based on extracting one-, two-, and three-layer features and
classifiers including k-NN, RF, LDC, and NB.
Te experimental results of one-layer feature extraction
are summarized in Table 2, which shows average recognition
rates from each of Sv1, Sv2, and Sv3 based on k-NN, RF, LDC,
and NB on the three datasets. Te experimental results of
two-layer feature extraction are summarized in Table 3,
which shows average recognition rates from each of
Sv1 + Sv2, Sv1 + Sv3, and Sv2 + Sv3 based on k-NN, RF, LDC,
and NB on the three datasets. Te experimental results of
three-layer feature extraction are summarized in Table 4,
which shows recognition rates from Sv1 + Sv2 + Sv3 based on
k-NN, RF, LDC, and NB on the three datasets; the same
experiment was conducted five times. Table 5 shows a
comparison of recognition rates for one-, two-, and three-
layer feature extraction based on k-NN, RF, LDC, and NB on
the three datasets.
As can be seen from Tables 2 and 3, although the rec-
ognition rates based on one or two layers are relatively good,
they are not very stable. According to Table 4, the recog-
nition rate with the extracted three-layer features is relatively
stable. As shown in Table 5, the performance of three-layer
feature extraction based on the classifiers is superior to that
of one or two layers, demonstrating that the representation
generated by three-layer features has high discrimination.
Moreover, it was found that our three-layer features based
on these classifiers achieve satisfactory recognition perfor-
mance on the three datasets. In addition, the recognition rate
of three-layer features based on these classifiers is higher
than that of two-layer features on the three datasets, and that
of two-layer features based on these classifiers is higher than
that of one-layer features on the three datasets. For example,
…
y
AVG
y1
y2
y3
y4
GNFX1
GNFX2
GNFX3
GNFX4
Divided into
Four
subregions
K-dim
Softmax vector
of each subregion
Sv
…
…
…
…
…
Figure 3: Illustration of generating sparse softmax vector for four
subregions.
Journal of Electrical and Computer Engineering
5
All of y
AVG
Sv3
Sv3
Sv2
Sv2
Sv1
Sv1
Divided into
16 subregions
Divided into
Four
subregions
AVG
y
Softmax vector
of each subregion
Concatenate
K-dim
y1
y4
y1
y2
y16
GNFX
GNFX1
GNFX4
GNFX1
GNFX2
GNFX16
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
Figure 4: Illustration of three-layer feature extraction.
Input: Training sample X, and a testing sample y, and K-class subjects
Output: [Sv1, Sv2, Sv3]
(1)
let Sv1 � GNFX(y, w, K)
(2)
dividing X into 4 subregions: X1, X2, X3, X4
(3)
dividing y into 4 subregions: Y1, Y2, Y3, Y4
(4)
for each i in {1, 2, 3, 4} do
(5)
let Si
v � GNFXi(yi, w, K)
(6)
end for
(7)
let Sv2 � 1/4 􏽐4
i�1 Si
v
(8)
dividing X into 16 subregions:X1, X2, X3, . . . , X16
(9)
dividing X into 16 subregions: Y1, Y2, Y3, . . . , Y16
(10)
for each j in {1, 2, 3, . . . , 16} do
(11)
let Sj
v � GNFXj(yj, w, K)
(12)
end for
(13)
let Sv3 � 1/16􏽐16
j�1Sj
v
(14)
output [Sv1, Sv2, Sv3]
ALGORITHM 2: Tree-layer feature extraction.
Table 1: Relevant parameters of three datasets used in this work.
Dataset
Number of subjects
Fs (Hz)
Length (min)
BIDMC
53
125
8
CapnoBase
42
300
8
MIMIC
32
125
10
Table 2: Average recognition rates for each of Sv1, Sv2, and Sv3.
Dataset
Classifier
Average recognition rate (%)
Sv1
Sv2
Sv3
BIDMC
k-NN
99.76
99.89
99.93
RF
95.77
95.07
92.41
LDC
91.74
84.73
76.49
NB
94.82
87.58
66.32
MIMIC
k-NN
93.69
96.93
99.78
RF
84.50
85.19
91.25
LDC
82.28
81.78
79.09
NB
95.28
95.19
89.94
CapnoBase
k-NN
99.54
99.72
99.95
RF
97.17
94.05
95.33
LDC
95.05
90.57
83.24
NB
98.91
97.96
93.98
Table 3: Average recognition rates for each of Sv1 + Sv2, Sv1 + Sv3,
and Sv2 + Sv3.
Dataset
Classifier
Average recognition rate (%)
Sv1 + Sv2
Sv1 + Sv3
Sv2 + Sv3
BIDMC
k-NN
99.88
99.88
100.0
RF
96.94
97.29
95.59
LDC
97.54
97.15
94.45
NB
98.20
96.89
90.96
MIMIC
k-NN
95.75
96.31
98.47
RF
88.97
92.91
89.31
LDC
92.31
91.41
91.25
NB
96.54
97.03
96.13
CapnoBase
k-NN
99.79
99.60
99.95
RF
97.60
98.67
96.12
LDC
98.15
97.79
94.93
NB
99.81
99.76
99.05
6
Journal of Electrical and Computer Engineering
the proposed method achieves recognition rates of 99.73%,
99.78%, and 99.92% for one, two, and three layers based on
k-NN on the CapnoBase dataset, respectively. In short, all
these results demonstrate the effectiveness of the three-layer
feature extraction method. As can be also seen from Table 5,
the recognition rate of three-layer features based on k-NN is
obviously better than that based on RF, LDC, and NB on the
three datasets. It indicates that k-NN has advantages for
small-scale data. In addition, small-scale data were used in
the literature [14–16], and the corresponding recognition
rate was only 96%, which showed that the performance of
deep learning drops apparently on small-scale data.
Furthermore, additional experiments were conducted to
analyze the influence of data length in feature extraction.
First, 20 subjects were randomly selected from each dataset
for the three datasets. Second, continuous data of 0.5, 1, 1.5,
and 2 cycles were blindly selected for each subject, 80% of
which were used for training and 20% for testing, and three-
layer features were extracted from the continuous data.
Finally, Figure 5 shows the influence of data length on the
proposed method, from which it can be observed that the
best recognition rate is achieved when the data is 1.5 or 2
cycles in length. When data of 0.5 cycle length are used, the
recognition rate fluctuates greatly, but when those of 1 cycle
length are selected, the recognition rate is clearly improved.
However, when the data length is sufficiently long, the
recognition rate can be improved, but the effect is not
obvious, and it will cost a lot of time.
3.2.3. Comparisons with the State-of-the-Art Methods.
From the current research on PPG biometric recognition,
several methods use their datasets [1, 2, 4, 5, 7, 12], while
others use, e.g., OpenSignal [9], Biosec [9, 16], and TROIKA
[6, 14, 15], yet the BIDMC and MIMIC datasets are not
involved in this literature. Terefore, comparisons between
our method and other state-of-the-art methods on the
CapnoBase dataset are summarized in Table 6. Compared
with the other methods, our method need not extract feature
points from raw PPG data, and the data only needs to simply
be rerepresented by a sliding window, so our method has the
advantages of simple operation and low time complexity.
Tree-layer features based on softmax vector are extracted
only from blindly taking the continuous data of more than 1
cycle length, in which we consider the combination of
macro- and microdata. Te experiments show that our
method achieves very high accuracy and has strong ro-
bustness, and the microdata tend to show more detailed and
discriminative information than the macrodata.
3.2.4. Time-Cost Analysis. To verify the efficiency of the
proposed framework, the preprocessing time (in s), feature-
extraction time, and matching time cost during the PPG
biometric recognition procedure on the CapnoBase dataset
are further summarized. Te results are summarized in
Table 7. Te experimental environment includes an HP
EliteBook 8570w notebook with an Intel (R) Core (TM) i7-
3740QM CPU @ 2.7 GHz with 2.70 GHz, 8.00 GB RAM, and
the 64-bit Windows 7 operating system. Table 7 lists the
average preprocessing time per sample, average feature-
extraction time per sample, and average matching time per
Table 4: Average recognition rates based on Sv1 + Sv2 + Sv3 re-
peating all experiments five times.
Dataset
Classifier
Average recognition rate (%) based on
Sv1 + Sv2 + Sv3
1
2
3
4
5
BIDMC
k-NN
100.0
99.98
99.75
100.0
100.0
RF
98.77
96.98
97.92
98.21
98.49
LDC
99.15
98.68
98.58
99.72
98.68
NB
98.68
97.74
98.76
98.96
99.15
MIMIC
k-NN
96.56
98.81
95.69
96.84
98.13
RF
92.97
91.56
89.22
94.84
88.91
LDC
95.31
94.84
93.28
89.69
90.00
NB
96.25
97.5
98.91
94.38
98.69
CapnoBase
k-NN
100.0
99.9
99.95
100.0
99.76
RF
97.24
97.9
98.05
98.33
96.43
LDC
98.24
98.29
97.67
99.24
98.43
NB
99.52
99.67
99.86
99.81
99.71
Table 5: Comparison of recognition rates of one-, two-, and three-
layer feature extraction.
Dataset
Classifier
Average recognition rate (%)
One layer
Two layers
Tree layers
BIDMC
k-NN
99.86
99.92
99.95
RF
94.42
96.61
98.07
LDC
84.32
96.38
98.96
NB
82.91
95.35
98.66
MIMIC
k-NN
96.80
96.84
97.21
RF
86.98
90.40
91.50
LDC
81.05
91.65
92.62
NB
93.47
96.57
97.15
CapnoBase
k-NN
99.73
99.78
99.92
RF
95.52
97.46
97.59
LDC
89.62
96.95
98.37
NB
96.95
99.54
99.71
Recognition rate (%)
100.00
80.00
60.00
40.00
20.00
0.00
BIDMC
MIMIC
CapnoBase
k-NN
k-NN
RF
RF
RF
DAC
DAC
DAC
NB
NB
NB
k-NN
0.5 cycles
1 cycle
1.5 cycles
2 cycles
Figure 5: Influence of data length on the proposed method.
Journal of Electrical and Computer Engineering
7
sample pair. Since there is no analysis of time cost in other
studies, only our experimental results are summarized here.
From Table 7, it is found that the feature-extraction time of
the proposed method is fast and acceptable. In addition, the
proposed method is more efficient than deep-learning-based
methods, as it is well known that deep-learning-based
training is time-consuming. In conclusion, the proposed
method is efficient and scalable.
4. Conclusions and Directions of Future Work
In this study, a PPG biometric recognition framework is
proposed. First, raw PPG data are rerepresented by sliding
window scanning, which is not sensitive to noise; the raw
PPG data can be blindly segmented, and then the raw PPG
data do not need other complicated denoising operation.
Second, the three-layer extracted features combine global
and local subfeatures, and each layer feature is represented
by a sparse softmax vector. Finally, the extensive experi-
mental results demonstrate that the method based on three-
layer features and k-NN can achieve high recognition rates
of 99.95%, 97.21%, and 99.92% on the BIDMC, MIMIC, and
CapnoBase datasets, respectively, and three-layer features
can also achieve high recognition rates using RF, LDC, and
NB on the three datasets. Te extensive experimental results
on the three datasets also demonstrate that our method
outperforms several state-of-the-art methods. In particular,
our method is suitable for small-scale PPG data biometric
recognition and consumes fewer resources. Despite the
satisfactory performance achieved by our method, there is
still some room for the proposed PPG biometric recognition
framework, especially for large-scale PPG applications. In
our future work, we will further explore the attributive
information of the PPG signal to improve the performance.
Data Availability
Te simulated data used to support the simulation part of
this study are available from the corresponding author upon
request, and the real-world PPG data can be obtained from
https://www.physionet.org/physiobank/database/bidmc, https://
archive.physionet.org/cgi-bin/ATM?database�mimic2db,
and
http://www.capnobase.org/database/pulse-oximeter-ieee-tbme-
benchmark.
Conflicts of Interest
Te authors declare that there are no conflicts of interest
regarding the publication of this paper.
Acknowledgments
Te authors thank LetPub (http://www.letpub.com) for its
linguistic assistance during the preparation of this manu-
script. Tis work was supported in part by the NSFC-
Xinjiang Joint Fund under Grant U1903127 and in part by
the Key Research and Development Project of Shandong
Province under Grant 2018GGX101032.
References
[1] Y. Y. Gu, Y. Zhang, and Y. T. Zhang, “A novel biometric
approach in human verification by photoplethysmographic
signals,” in Proceedings of the International IEEE EMBS
Special Topic Conference on Information Technology Appli-
cations in Biomedicine, pp. 13-14, Birmingham, UK, 2003.
[2] A. R. Kavsaoglu, K. Polat, and M. R. Bozkurt, “A novel feature
ranking algorithm for biometric recognition with PPG sig-
nals,” Computers in Biology and Medicine, vol. 49, pp. 1–14,
2014.
[3] S. Chakraborty and S. Pal, “Photoplethysmogram signal based
biometric recognition using linear discriminant classifier,” in
Proceedings of the IEEE 2nd International Conference on
Control, Instrumentation, Energy & Communication (CIEC),
pp. 183–187, Kolkata, India, January 2016.
[4] A. R. Kavsao˘glu, K. Polat, and M. R. Bozkurt, “Feature ex-
traction
for
biometric
recognition
with
photo-
plethysmography signals,” in Proceedings of the 2013 21st
Signal Processing and Communications Applications Confer-
ence (SIU 2013), pp. 1–4, Haspolat, Turkey, April 2013.
[5] A. Al-Sidani, B. Ibrahim, and A. Cherry, “Biometric identi-
fication using photoplethysmography signal,” in Proceedings
of the Tird International Conference on Electrical and Bio-
medical Engineering, Clean Energy and Green Computing
(EBECEGC 2018), pp. 12–15, Beirut, Lebanon, April 2018.
[6] V. Jindal, J. Birjandtalab, and M. B. Pouyan, “An adaptive
deep learning approach for PPG-based identification,” in
Proceedings of the 2016 38th Annual International Conference
of the IEEE Engineering in Medicine and Biology Society
(EMBC’16), pp. 6401–6404, Orlando, FL, USA, August 2016.
[7] N. I. Nadzri, K. A. Sidek, and A. F. Ismai, “Biometric rec-
ognition for twins inconsideration of age variability using
PPG signals,” Journal of Telecommunication, Electronic and
Computer Engineering, vol. 10, pp. 97–100, 2018.
[8] J. Sancho, ´A. Alesanco, and J. Garc´ıa, “Biometric authenti-
cation using the PPG: a long-term feasibility study,” Sensors,
vol. 18, no. 5, p. 1525, 2018.
[9] P. Spachos, J. Gao, and D. Hatzinakos, “Feasibility study of
photoplethysmographic signals for biometric identification,”
in Proceedings of the IEEE 2011 17th International Conference
on Digital Signal Processing (DSP), pp.1–5, Corfu, Greece, July
2011.
Table 7: Time costs of the proposed method.
Process
Time (s)
Preprocessing
0.0001
Feature extraction
0.4421
Matching
0.0013
Table 6: Comparison of the proposed method and state-of-the-art
methods on the CapnoBase dataset.
Method
Matching
Accuracy (%)
EER (%)
[10]
k-NN
99.84
1.31
[13]
RF
99.00
—
[8]
Manhattan and
Euclidean distances
—
1.00
Proposed
k-NN
99.92
—
8
Journal of Electrical and Computer Engineering
[10] N. Karimian, M. Tehranipoor, and D. Forte, “Non-fiducial
PPG-based authentication for healthcare application,” in
Proceedings of the 2017 IEEE EMBS International Conference
on Biomedical Health Informatics (BHI), pp. 429–432, Orland,
FL, USA, February 2017.
[11] U. Yadav, S. N. Abbas, and D. Hatzinakos, “Evaluation of PPG
biometrics for authentication in different states,” in Pro-
ceedings of the IEEE 2018 International Conference on Bio-
metrics (ICB), pp. 277–282, Gold Coast, Australia, February
2018.
[12] P. Farago, R. Groza, and L. Ivanciu, “A correlation-based
biometric identification technique for ECG, PPG and EMG,”
in Proceedings of the 2019 15th International Conference on
Telecommunications (ConTEL), pp. 716–719, Graz, Austria,
July 2019.
[13] S.-W. Lee, D. K. Woo, and Y. K. Son, “Wearable bio-sig-
nal(PPG)-based personal authentication method using ran-
dom forest and period setting considering the feature of PPG
signals,” Journal of Computers, vol. 14, no. 4, pp. 283–294,
2019.
[14] L. Everson, D. Biswas, M. Panwar et al., “BiometricNet: deep
learning based biometric identification using wrist-worn
PPG,” in Proceedings of the 2018 IEEE International Sym-
posium on Circuits and Systems (ISCAS), pp. 1–5, Florence,
Italy, May 2018.
[15] D. Biswas, L. Everson, M. Liu et al., “CorNET: deep learning
framework for PPG-based heart rate estimation and biometric
identification in ambulant environment,” IEEE Transactions
on Biomedical Circuits and Systems, vol.13, no. 2, pp. 282–291,
2019.
[16] D. Y. Hwang and D. Hatzinakos, “PPSNet: PPG-based per-
sonalized verification system,” in Proceedings of the 2019 IEEE
Canadian Conference of Electrical and Computer Engineering
(CCECE), pp. 1–4, Edmonton, Canada, May 2019.
[17] J. Wright, A. Y. Yang, and A. Ganesh, “Robust face recog-
nition via sparse representation,” IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, vol. 31, no. 2,
pp. 210–227, 2009.
[18] Z. H. Sastry and J. Feng, “Deep forest: toward an alternative to
deep neural network,” in Proceedings of the Twenty-Sixth
International Joint Conference on Artificial Intelligence (IJCAI-
17), pp. 3553–3559, Melbourne, Australia, August 2017.
[19] J. Yang and Y. Zhang, “Alternating direction algorithms for
L1-problems in compressive sensing,” SIAM Journal on Sci-
entific Computing, vol. 33, no. 1, pp. 250–278, 2011.
[20] J. Liu and L. Zhang, “Sparse softmax vector coding based deep
cascade model,” in Proceedings of the Chinese Conference on
Computer Vision (CCCV2017), pp. 603–614, Tianjin, China,
October 2017.
[21] L. Zhang, J. Liu, B. Zhang et al., “Deep cascade model-based
face recognition: when deep-layered learning meets small
data,” IEEE Transactions on Image Processing, vol. 29,
pp. 1016–1029, 2019.
[22] J. Dumont, L. Luo, J. Qian et al., “Nuclear norm based matrix
regression with applications to face recognition with occlu-
sion and illumination changes,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 39, no. 1, pp. 156–171,
2016.
[23] L. Luo, J. Yang, J. Qian et al., “Robust image regression based
on the extended matrix variate power exponential distribution
of dependent noise,” IEEE Transactions on Neural Networks
and Learning Systems, vol. 28, no. 9, pp. 2168–2182, 2016.
[24] J. Gou, L. Wang, Z. Yi, and Q. Mao, “Weighted discriminative
collaborative competitive representation for robust image
classification,” Neural Networks, vol. 125, pp. 104–120, 2020.
[25] J. Yuan, W. Qiu, and Z. Yi, “Locality constrained represen-
tation-based k-nearest neighbor classification,” Knowledge-
Based Systems, vol. 167, pp. 38–52, 2019.
[26] N. A. L. Jaafar, K. A. Sidek, and S. N. A. M. Azam, “Accel-
eration plethysmogram based biometric identification,” in
Proceedings of the IEEE 2015 International Conference on
BioSignal
Analysis,
Processing
and
Systems
(ICBAPS),
pp. 16–21, Kuala Lumpur, Malaysia, May 2015.
[27] C. Chen, A. Liaw, and L. Breiman, Using Random Forest to
Learn Imbalanced Data, Vol. 110, University of California,
Berkeley, CA, USA, 2004.
[28] M. A. F. Pimentel, A. E. W. Johnson, P. H. Charlton et al.,
“Toward a robust estimation of respiratory rate from pulse
oximeters,” IEEE Transactions on Biomedical Engineering,
vol. 64, no. 8, pp. 1914–1923, 2017.
[29] A. L. Birrenkott, L. Amaral, L. Glass et al., “PhysioBank,
PhysioToolkit, and PhysioNet: components of a new research
resource
for complex physiologic signals,” Circulation,
vol. 101, no. 23, pp. 215–220, 2000.
[30] https://archive.physionet.org/cgi-bin/ATM?database�mimic2db.
[31] G. B. Moody and R. G. Mark, “A database to support de-
velopment and evaluation of intelligent intensive care
monitoring,” in Proceedings of the Computing in Cardiology
Conference, pp. 657–660, Indianapolis, IN, USA, September
1996.
[32] http://www.capnobase.org/database/pulse-oximeter-ieee-tbme-
benchmark.
[33] W. Karlen, S. Raman, and J. M. Ansermino, “Multiparameter
respiratory rate estimation from the photoplethysmogram,”
IEEE Transactions on Biomedical Engineering, vol. 60, no. 7,
pp. 1946–1953, 2013.
Journal of Electrical and Computer Engineering
9
