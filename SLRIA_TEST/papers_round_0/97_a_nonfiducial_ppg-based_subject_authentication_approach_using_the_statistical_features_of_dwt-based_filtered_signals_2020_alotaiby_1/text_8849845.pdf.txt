Research Article
A Nonfiducial PPG-Based Subject Authentication Approach Using
the Statistical Features of DWT-Based Filtered Signals
Turky N. Alotaiby
,1 Fatima Aljabarti,2 Gaseb Alotibi,3 and Saleh A. Alshebeili4
1KACST, Saudi Arabia
2Prince Sultan University, Saudi Arabia
3Public Security, Saudi Arabia
4Department of Elect. Engineering, King Saud University, Saudi Arabia
Correspondence should be addressed to Turky N. Alotaiby; totaiby@kacst.edu.sa
Received 2 March 2020; Revised 25 August 2020; Accepted 21 September 2020; Published 17 October 2020
Academic Editor: Davide Palumbo
Copyright © 2020 Turky N. Alotaiby et al. This is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is
properly cited.
Nowadays, there is a global change in lifestyle that is moving more toward the use of e-services and smart devices which necessitate
the verification of user identity. Different organizations have put into place a range of technologies, hardware, and/or software to
authenticate users using fingerprints, iris recognition, and so forth. However, cost and reliability are significant limitations to the
use of such technologies. This study presents a nonfiducial PPG-based subject authentication system. In particular, the
photoplethysmogram (PPG) signal is first filtered into four signals using the discrete wavelet transform (DWT) and then
segmented into frames. Ten simple statistical features are extracted from the frame of each signal band to compose the feature
vector. Augmenting the feature vector with the same features extracted from the 1st derivative of the corresponding signal is
investigated, along with different fusion approaches. A support vector machine (SVM) classifier is then employed for the
purpose of identity authentication. The proposed authentication system achieved an average authentication accuracy of 99.3%
using a 15 sec frame length with the augmented multiband approach.
1. Introduction
Today, with an increasing dependency on the information
systems, identity authentication has become an essential part
of life. From accessing mobile phones to performing online
financial transactions, a user needs to authenticate his/her
identity. Generally, there are three approaches for user
authentication: (1) soft keys (e.g., passwords), (2) hard keys
(e.g., smart cards), and (3) biometrics [1]. Traditional authen-
tication techniques such as account passwords and smart
cards are still widely used, but they are not reliable enough
because they are easily stolen, lost, forgotten, and forged. In
the recent years, biometric-based identity authentication has
been gaining more attention. The International Organization
for Standardization (ISO) defined biometrics as the auto-
mated recognition of individuals based on their behavioral
and biological characteristics [2]. Biometric-based identity
authentication is aimed at uniquely identifying individuals
based on their physiological and/or behavioral characteristics
such as fingerprint, face, retina, palm print, lip movement,
gait, DNA, voice, EEG, or ECG [2–19].
In 2003, Gu et al. presented the first attempt at using a PPG
signal for identity authentication [20, 21]. Hertzman and Spiel-
man in 1973 described the PPG [22] and a typical PPG device,
which consists of two parts: the light source and the photode-
tector. The light source emits light to be reflected off tissue
while the photodetector detects and measures the reflected
light, which is proportional to blood volume variation [23].
Different light colors are used for different applications, where
the most popular colors are red and green [24]. The PPG-based
identity authentication technique has diverse advantages over
other biometric approaches because it is easy to set up, is sim-
ple to implement, has low cost, and could be placed comfort-
ably in different parts of the body such as the finger or wrist.
Hindawi
Journal of Sensors
Volume 2020, Article ID 8849845, 14 pages
https://doi.org/10.1155/2020/8849845
The biometric identity authentication system consists of
two main phases: enrollment and authentication. The pre-
processing and feature extraction stages are common in both
phases. In the authentication phase, the decision stage is
usually a classification process. Many researchers proposed
different PPG-based identity authentication methods using
various feature extraction approaches such as fiducial, nonfi-
ducial, and hybrid with the utilization of diverse classifiers.
For example, Gu et al. [20] used four fiducial features: the
peak number, the upward/downward slopes, and the time
interval. A feature vector template was then formulated for
each subject, and the decision was made using Euclidian dis-
tance. With a dataset of 17 healthy subjects, they achieved
94% identification accuracy. Furthermore, the authors tried
a fuzzy-based classification approach [21]. Kavsaoğlu et al.
[24] presented a fiducial-based recognition system using 40
features extracted from PPG’s signal first and second deriva-
tives and KNN classifier. They applied their method on a 15-
period PPG signal belonging to 30 healthy subjects, where the
best result they achieved was an accuracy of 94.44%. Sarkar
et al. [25] proposed a dynamical model-based authentication
approach, which maps each cardiac cycle to a limit cycle,
which facilitates the decomposition of the PPG signal to a
sum of Gaussian functions, where the Gaussian parameters
serve as feature templates. Using the data of 23 subjects of
the DEAP dataset [26], they achieved identification accura-
cies of 90% and 95% with 2 and 8 seconds of PPG test signal
data, respectively. Jindal et al. [27] proposed a two-stage
identification system. In the first stage, the individuals are
clustered into different groups, and then, Boltzman Machines
and Deep Belief Networks are used for the identification.
They tested their approach using 12 subjects of the Troika
dataset [28] and achieved an identification accuracy of
96.1%. Choudhary and Manikandan [29] presented a
template matching identity authentication approach using
normalized cross-correlation. They tested their method on
30 subjects and achieved an average false rejection rate
(FRR) of 0.32 and false acceptance rate (FAR) of 0.32. Luque
et al. [30] presented a nonfiducial subject authentication
approach based on convolutional neural networks and evalu-
ated their approach using datasets PulseID and Troika with
accuracies of 78.2% and 83.2%, respectively. Yadav et al.
[31] proposed a template matching identity authentication
approach using Continuous Wavelet Transform (CWT) for
feature extraction and linear discriminant analysis for feature
dimensionality reduction. They achieved an equal error rate
of 0.46% using the CapnoBase [32] dataset. Biswas et al.
[33] presented a subject identification approach using a
four-layer deep neural network. They tested their approach
using 22 subjects of the Troika dataset and achieved an aver-
age accuracy of 96%.
In this study, we present a new nonfiducial approach for
subject identity authentication which relies on simple statis-
tical features of DWT-based filter signals of the PPG signal
and a support vector machine classifier. The PPG signal is
first filtered into four signals and then segmented into
frames. The first derivative of each DWT frame is also com-
puted. Different feature fusion approaches are also studied. It
has been found that the proposed authentication system can
have an average authentication accuracy of 99.30% with a
frame length of 15sec when the feature vector is composed
of features extracted from frames of all band signals and their
corresponding 1st derivatives. This result outperforms the
best-performing methods in the literature, as will be demon-
strated in Section 3. The paper is structured as follows:
Section 2 presents the proposed system, the performance
evaluation is presented in Section 3, and Section 4 contains
the concluding remarks.
2. System Model
The model of the proposed authentication system is shown in
Figure 1. It includes signal acquisition, signal framing, and
feature extraction stages in both the enrollment and authen-
tication phases. In signal framing, three tasks are performed:
signal smoothing, signal filtering, and signal segmentation.
The subject authentication stage makes use of a SVM classi-
fier for identity authentication through a one-versus-all clas-
sification approach. In this work, the classifier is trained and
evaluated using the CapnoBase dataset which is the largest
publically available dataset. This dataset was developed at
the University of British Columbia, Vancouver, Canada, in
2009 and contains respiratory signals obtained from capno-
graphy and spirometry. It also includes the CO2 and photo-
plethysmogram
(PPG)
waveforms
of
42
subjects:
29
children (median age: 8.7, range 0.8−16.5 years) and 13
adults (median age: 52.4, range 26.2−75.6 years). The pulse
peaks from PPG and breaths from CO2 are annotated by
experts. Each subject has a recording session of 8-minute
length using a sampling frequency of 300 Hz [32].
2.1. Signal Framing. In the signal framing stage, three tasks
are performed: PPG signal smoothing, filtering the PPG sig-
nal using DWT, and signal segmentation. A moving median
filter with a window size of 44 samples, selected based on
extensive experimentation, was employed for smoothing
the PPG signal. The filtered DWT signals are then segmented
into frames with a nonoverlapping sliding window of size 1,
3, 5, 7, 10, or 15sec. Different frame lengths are selected,
along the lines of other works available in the literature [25,
29], to study the effect of frame length on the authentication
system irrespective of individual PPG cycle. Table 1 shows
the frame length in seconds and samples, as well as the num-
ber of related frames.
Figure 2 presents the frequency spectrum of the PPG
signals averaged over all subjects considered in this study.
As can be seen in the figure, the spectral energy is mainly
concentrated on the frequencies below 12Hz. Therefore, we
use multiresolution wavelet transform to decompose the
PPG signal into four bands covering the range from 0.1 to
18 Hz. The subbands are then extracted by passing the PPG
signal through an iterated filter bank, as shown in Figure 3.
In this work, the four subband coefficients, namely, d5, d6,
d7, and a7, are estimated using the second member of the
Daubechies wavelet family [34, 35]. Figure 4 shows the
frequency responses of the corresponding subband filters.
The coefficients of the four subbands are used to recon-
struct the four filtered signals. Here, the symbols B1, B2, B3,
2
Journal of Sensors
and B4 are used to denote the four filtered signals which are
segmented using different frame lengths, as mentioned
previously.
2.2. Feature Extraction and Fusion. This subsection discusses
the feature extraction approaches adopted in this work. The
feature vector is formed by extracting 10 features from each
preprocessed frame. The extracted features are the mean,
median, variance, standard deviation, interquartile range,
the first quarter (Q1), the third quarter (Q3), kurtosis, skew-
ness, and entropy. The definitions of these features are well
known and can be found in [36, 37]. We consider eight cases
to form the feature vector as follows:
(1) Feature vector V1, extracted directly from the time
domain PPG signal
(2) Feature vector V2, extracted from the 1st derivative of
the time domain PPG signal
(3) Feature vector V12, which is the fusion of V1 and V2
(4) Feature vector Wi, extracted from the ith filtered
signal of the DWT of the PPG signal (i = 1, 2, 3, 4)
(5) Feature vector Wi‐j, which is the fusion of Wi, Wi+1,
⋯, W j ði, j = 1,2,3,4, j > iÞ
(6) Feature vector Di, extracted from the 1st derivative of
the ith filtered signal of the DWT of the PPG signal
(7) Feature vector Gi, which is the fusion of Wi and Di
(8) Feature vector Gi‐j, which is the fusion of Gi+1, Gi+2,
⋯, Gj ði, j = 1,2,3,4Þ
The main motivation behind the computation of all these
feature vectors is to conduct a comprehensive investigation
for the purpose of determining the most influential features
for the authentication process. Figure 5 shows the method
used to generate the feature vectors V1, V2 and the fused fea-
ture vector V12. Figure 6(a), on the other hand, is a box plot
showing the distribution values of the statistical features
extracted from a PPG signal framed in 5 sec length of subject
1, while Figure 6(b) presents the distribution values of the
statistical features extracted from the 1st derivative of the
PPG signal of the same subject. Noticeably, most of the fea-
tures’ values extracted from the 1st derivative are near zero
and with low dispersion.
The capability of these features to separate subjects is
examined using the t-distribution stochastic neighbor embed-
ding (t-SNE) algorithm [38], which is commonly used for data
dimensionality reduction. The t-SNE algorithm preserves
both the local and global structures of data; hence, it facilitates
its visual inspection. For illustration purposes, we apply the t
-SNE algorithm to the features extracted from data frames of
seven subjects with a frame length of 5 sec to show the separa-
bility of the subjects’ frames, as depicted in Figure 7. Subject 2
and subject 5 are well separated from other subjects. However,
subject 1, subject 6, and subject 7 are less separable, as is also
the case with subject 3 and subject 4.
Figure 8 presents the results of applying the t-SNE algo-
rithm to the augmented (concatenated) features extracted
from the same seven subjects. The figure demonstrates that
the subjects’ frame separability is now better, which indicates
that augmentation would improve the classification stage.
Signal
acquisition
Signal framing
Enrollment phase
Authentication phase
Feature
extraction/fusion
Registered
PPG features
Classification
Training data
Accept/reject
f11
f1n
fmn
fm1
Figure 1: System model.
Table 1: Different signals’ framing length.
Frame length in
seconds
Frame length in
samples
Number of
frames
1
300
480
3
900
160
5
1500
96
7
2100
68
10
3000
48
15
4500
32
3
Journal of Sensors
Figure 9 shows the methods used to generate the feature
vectors Wi, Wi‐j, Di, Gi, and Gi‐j. We construct the feature
vector via four ways: single filtered signal (Wi), multifiltered
signals (Wi‐j), augmented single filtered signal (Gi), and
augmented multifiltered signals (Gi‐j). In the single filtered
signal, the features are extracted from a specific filtered signal
of a specific band while in multifiltered signals, the features
are
extracted
from
multifiltered
signals
and
fused
(concatenated) to compose a feature vector, as shown in
Figure 9. Also, we study the effect of augmenting the
extracted features with features of the 1st derivative of the
corresponding signal.
Figure 10 contains boxplots showing the filtered signals’
feature value distribution. Notice that there are variations
in the features’ values among the different bands.
Applying the t-SNE algorithm to features extracted from
a single filtered signal of the seven subjects mentioned before
with a frame length of 5 sec shows that the subjects’ frames
are not well separable, as demonstrated in Figure 11. How-
ever, using multifiltered signals improved the separability of
subjects’ frames. The augmented multibands gave the best
separability as shown in Figure 12. It is relevant here to note
that the abbreviation B1‐4 means filtered signals 1, 2, 3, and 4.
2.3. Authentication. In the authentication phase, a one-
versus-all classification approach is adopted. The classifier
is trained on feature vectors extracted from the data of the
target subject and other subjects. In this case, the positive
instances will be much lower than the negative instances in
the training dataset. Therefore, to balance the two sample
sets, we employed an oversampling positive instance strategy
[39], which replicates the positive instances to match the neg-
ative instances. The classifier results are binary “1” for the
target subject and “0” otherwise. In this work, an SVM with
0
2
4
6
8
10
12
14
16
18
20
Frequency (Hz)
0
0.05
0.1
0.15
Magnitude
Figure 2: PPG signal frequency spectrum.
PPG
signal
d1
a1
d2
a2
d3
a3
d4
a4
d5
a5
d6
a6
d7
a7
H
L
Figure 3: Multilevel wavelet decomposition.
4
Journal of Sensors
a radial basis function (RBF) kernel is utilized as a classifier
[40, 41].
3. Performance Evaluation
In this section, the performance of the proposed approaches
is presented and discussed. Three widely used performance
metrics are considered: accuracy, equal error rate (EER),
and area under the curve (AUC) [31, 42]. The first metric is
defined
as
Accuracy = ðTP + TNÞ/ðTP + TN + FP + FNÞ,
where true positives (TP) (true negative (TN)) refers to a pos-
itive (negative) instance that is correctly classified as positive
(negative), and false positive (FP) (false negative (FN))
means a positive (negative) instance that is incorrectly classi-
fied as positive (negative). Furthermore, the equal error rate
is the error rate at which both the false positive rate and the
false negative rate are equal. The AUC is the area under the
receiver operating characteristic (ROC) curve created by
plotting the true positive rate against the false positive rate
at various thresholding settings.
The results are obtained using the CapnoBase dataset,
which is composed of 42 subjects. The feature vectors are
extracted from the subjects’ frames and divided into training
and testing sets. The training set consists of 60% of the fea-
ture vectors of each subject, while the remaining 40% is used
to evaluate the trained model.
Figure 13 presents the performance results using different
frame lengths when the features are extracted directly from
0
5
10
15
20
Frequency (Hz)
0
2
4
6
8
10
12
Magnitude
d5
d6
d7
a7
Figure 4: Frequency response of the wavelet filter bank.
Signal framing
Feature
extraction
PPG signal
Extracted features
V1 =
f11
f1n
fmn
fm 1
(a)
Signal framing
Feature
extraction
v1
v2
v12 =
1st derivative 
Feature
extraction 
Fused features
PPG signal
Feature fusion
f11
f1nd11
d1n
fmn dm 1
dmn
fm1
(b)
Figure 5: (a) Generation of V1; (b) generation of V12.
5
Journal of Sensors
–5
Features
5
10
15
20
25
30
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
(a)
2
4
0
Features
8
6
10
12
14
16
18
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
(b)
Figure 6: Box plots of the statistical features extracted from (a) PPG signal (V1) and (b) 1st derivative (V2).
–25
–20
–15
–10
–5
0
5
10
15
20
–20
–10
0
10
20
30
40
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
Figure 7: Subjects’ frame separability using V1.
6
Journal of Sensors
–30
–20
–10
0
10
20
30
40
–25
–20
–15
–10
-5
0
5
10
15
20
25
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
Figure 8: Subjects’ frame separability using V12.
Figure 9: Generation of feature vectors Wi, Wi‐j, Di, Gi, and Gi‐j.
7
Journal of Sensors
the PPG signal’s frames. The approach achieved an average
accuracy greater than 93% when the frame length is equal
to or greater than 3 seconds. The best average accuracy of
95.03% is achieved using a 15sec frame length. Augmenting
the feature vector with the same 10 features extracted from
the 1st derivative of the signal improved the performance
results, as illustrated in Figure 13. The augmented method
achieved an average accuracy greater than 93.9% with all
the frame lengths and achieved the best average accuracy of
97.89% using a 15 sec frame length.
Figure 14 presents the performance results in terms of
accuracy when extracting the features from a single filtered sig-
nal and multifiltered signals (Bi‐j means bands Bi, Bi+1, ⋯, Bj).
Whenonlyasinglefilteredsignalisconsidered,thebestaverage
authentication accuracy of 90.16% is achieved when using the
fourth filtered signal (B4) with a 15sec frame length. Gener-
ally, extracting and fusing features from multisignals yield
better performance results. Extracting features from the
filtered signals 1, 2, 3, and 4 (B1‐4) achieved an average
accuracy of 91.88, 96.79%, 97.77%, 98.17%, 98.48, and
98.69% with 1, 3, 5, 7, 10, and 15 sec frame lengths, respec-
tively. Figure 15 shows the relationship between the
segment length and the achieved performance of extracting
features from multifiltered signals (B1‐4). It shows that as
the length of the frame increases, the accuracy also
increases. The EER of using a 1 sec frame length is more
than twice that obtained using a 15sec frame length. The
AUC of using a 1 sec frame length is >96%, while using a
15 sec frame length, it is more than 99%.
Figure 16 presents the performance results of augment-
ing the extracted features from a single filtered signal and
multifiltered signals with features from the 1st derivative of
the
corresponding
signal.
Augmented
features
(G34)
extracted from the third filtered signal (B3) and fourth
filtered signal (B4) achieved the highest average accuracies
of 94.6% and 94.24% with a 15 sec frame length. Augmented
features (G1, G2, G3, and G4) from the multisignals (B1‐4)
achieved an average accuracy of 94.27%, 98.09%, 98.82,
99.02%, 99.13%, and 99.3% with 1, 3, 5, 7, 10, and 15 sec
frame lengths, respectively. Figure 17 shows that the EER
and AUC are significantly improved using the augmented
features extracted from the multisignals (B1‐4).
–5
0
5
10
15
20
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
(a)
–2
0
1
5
7
8
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
–1
4
3 
2
6
(b)
–1
0
1
2
4
6
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
5
3
(c)
–1
1
2
6
8
9
Var
Kurtosis
Skewness
Median
Mean
iqr25
iqr75
std
Entropy
iqr
0
5
4 
3
7
(d)
Figure 10: Box plots of the statistical features extracted from the signals: (a) B1, (b) B2, (c) B3, and (d) B4.
8
Journal of Sensors
–40
–20
0
20
40
–20
–15
–10
–5
0
5
10
15
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(a) Features of B1
–40
–30
–20
–10
0
10
20
–30
–25
–10
0
10
20
30
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(b) Features of B2
–25
–20
–15
–10
–5
0
5
15
20
10
–40
–30
–20
–10
0
10
20
30
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(c) Features of B3
–40
–30
–20
–10
0
10
20
–30
–20
–10
0
20
10
30
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(d) Features of B4
Figure 11: Subjects’ frame separability using single filtered signal feature vector.
–30
–20
–10
0
10
20
30
40
–30
–20
–10
0
10
20
30
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(a) Fused features of all signal B1-4
–30
–20
–10
0
10
20
30
40
–30
–20
–10
0
10
20
30
Subject 1
Subject 2
Subject 3
Subject 4
Subject 5
Subject 6
Subject 7
(b) Fused features of all signals B1-4 with their corresponding 1st derivatives
Figure 12: Subjects’ frame separability using multifiltered signals’ feature vectors.
9
Journal of Sensors
Table 2 shows the performance of five classifiers named
SVM [40, 41], linear discriminant analysis (LDA) [43], Ran-
dom Forest (RF) [44], Naïve Bayes (NB) [45], and K-Nearest
Neighbor (KNN) [46] using features of B1‐4 a extracted from
a segment length of 15 sec. SVM and RF (with 100 decision
trees) have a similar accuracy performance while SVM has
the lowest EER among all classifiers.
Table 3 compares the results achieved using the proposed
approaches with the performance of identity authentication
methods which are available in the literature. The table
includes the method with its reference, the number of sub-
jects considered in the study, and the average authentication
accuracy. It is noteworthy that the augmented features
extracted from the four filtered signals achieved average
86
88
90
92
94
Accuracy
96
98
100
1
3
5
7
10
15
1
3
5
7
10
15
Frame's size (sec)
Features vector V1 
Features vector V12 
Figure 13: The authentication performance results using the feature vectors V1 and V12.
B1
B2
B3
B4
B1-3
B1-2
B1-4
B2-3
B2-4
B3-4
Frame’s size (sec)
Bands
1
3
5
7
10
15
95
90
85
70
Accuracy
75
80
Figure 14: Authentication performance results using the feature vectors Wi and Wi‐j (i, j = 1, 2, 3, 4). Note that Wi is the feature vector
extracted from the signal Bi, and Bi‐j is the concatenated feature vector extracted from the filtered signal i, i + 1, ⋯, j.
10
Journal of Sensors
Frame’s size (sec)
1
0.995
0.08
EER
AUC
0
1
3
5
7
10
15
0.07
0.06
0.05
0.4
0.03
0.02
0.99
0.985
0.98
0.975
0.97
0.965
EER (%)
AUC
Figure 15: Frame’s size and classification performance relation using features of signals B1‐4.
Figure 16: Authentication performance results using augmented features. Bi a means the ith filtered signal with feature vector Gi. Bi‐j a means
the ith till jth filtered signals with concatenated feature vectors Gi, Gi+1, ⋯, Gj.
Frame's size (sec)
EER
AUC
0
1
3
5
7
10
15
0.06
0.055
0.5
0.45
0.4
0.035
0.03
0.025
0.02
1
0.995
0.99
0.985
0.98
0.975
EER (%)
AUC
Figure 17: Frame length and classification performance relation using features of B1‐4 a.
11
Journal of Sensors
authentication accuracy greater than 99%. In the opinion of
the authors, the results obtained, which are more than 98%
with a 3 sec frame length and 99.3% with a 15sec frame
length, demonstrate that the proposed authentication system
is suitable for practical applications.
4. Conclusion
In this paper, a nonfiducial PPG-based subject authentication
system has been proposed which relies on statistical features
and support vector machine as a classifier. The photoplethys-
mogram (PPG) signal is first filtered into four signals using
the discrete wavelet transform (DWT) and then segmented
into frames. Ten simple statistical features are extracted from
the frame of each signal band to compose the feature vector.
Augmenting the feature vector with the same features
extracted from the 1st derivative of the corresponding signal
is investigated. In addition, different fusion approaches are
also investigated. A support vector machine (SVM) classifier
is employed for the purpose of identity authentication. The
proposed authentication system achieved an average accu-
racy of 91.88, 96.79%, 97.77%, 98.17%, 98.48, and 98.69%
with 1, 3, 5, 7, 10, and 15 sec frame lengths, respectively, with
features extracted from the filtered signals 1, 2, 3, and 4 (B1‐4).
The augmented multifiltered signal (B1‐4 a) method achieved
an average accuracy of 94.27%, 98.09%, 98.82, 99.02%,
99.13%, and 99.3% with 1, 3, 5, 7, 10, and 15 sec frame
lengths, respectively. Therefore, there is a trade-off between
latency and accuracy when the choice of appropriate frame
length depends on the intended application. The identity
authentication accuracy obtained is better than the highest
performing methods currently in the literature, as demon-
strated in Table 3. The investigation of different transforma-
tion domains and feature extraction methods with larger
datasets will be the topic of future work. In addition, there
is significant potential for exploring other application
domains of the utilization of PPG signals, as in a patient’s
diagnosis [47].
Data Availability
The data is available on this website (http://www.capnobase
.org/) and can be downloaded from it. It is also referenced
in this paper [32].
Conflicts of Interest
The authors declare that there are no conflicts of interest.
Acknowledgments
This work was supported by the Researchers Supporting
Project number (RSP-2020/46), King Saud University,
Riyadh, Saudi Arabia.
References
[1] L. O’Gorman, “Comparing passwords, tokens, and biometrics
for user authentication,” Proceedings of the IEEE, vol. 91,
no. 12, pp. 2021–2040, 2003.
[2] A. Bonissi, R. D. Labati, L. Perico, R. Sassi, F. Scotti, and
L. Sparagino, “A preliminary study on continuous authentica-
tion methods for photoplethysmographic biometrics,” in 2013
IEEE Workshop on Biometric Measurements and Systems for
Security and Medical Applications, pp. 28–33, Napoli, Italy,
September 2013.
[3] A. Fratini, M. Sansone, P. Bifulco, and M. Cesarelli, “Individ-
ual identification via electrocardiogram analysis,” BioMedical
Engineering OnLine, vol. 14, no. 1, p. 78, 2015.
[4] L. Hong, A. Jain, S. Pankanti, and R. Bolle, “Identity authen-
tication using fingerprints,” in Audio- and Video-based
Table 2: Classifier comparison results.
Band
Performance
SVM
LDA
RF
NB
KNN
B1‐4 a
Acc.
99.3
98.11
99.40
98.76
98.99
EER
0.02
0.06
0.04
0.28
0.14
AUC
1
0.99
0.99
0.95
0.98
Table 3: Performance comparison of the proposed approaches with methods in the literature.
Method
Subjects
Accuracy (%)
Fiducial features [20]
17
94
Fiducial features of 1st & 2nd derivatives [24]
30
94.44
Dynamical system model [25]
23
95
Clustering+Boltzman Machines+Deep Belief Networks [27]
12
96.1
Convolutional neural network [30]
43/20
78.2/83.2
Four-layer-deep neural network [33]
22
96
Proposed approaches
42
Augmented features from time domain signal
97.89
Augmented features from the four bands
99.3
12
Journal of Sensors
Biometric Person Authentication. AVBPA 1997. Lecture Notes
in Computer Science, vol 1206, J. Bigün, G. Chollet, and G.
Borgefors, Eds., pp. 1365–1388, Springer, Berlin, Heidelberg,
1997.
[5] W. Wenchao and S. Limin, “A fingerprint identification algo-
rithm based on wavelet transformation characteristic coeffi-
cient,” in 2012 International Conference on Systems and
Informatics (ICSAI2012), pp. 1–3, Yantai, China, May 2012.
[6] R. Ranjan, A. Bansal, J. Zheng et al., “A fast and accurate sys-
tem for face detection, identification, and verification,” IEEE
Transactions on Biometrics, Behavior, and Identity Science,
vol. 1, no. 2, pp. 82–96, 2019.
[7] J. H. Lai, P. C. Yuen, and G. C. Feng, “Face recognition using
holistic Fourier invariant features,” Pattern Recognition,
vol. 34, no. 1, pp. 95–109, 2001.
[8] N. K. Shaydyuk and T. Cleland, “Biometric identification via
retina scanning with liveness detection using speckle contrast
imaging,” in 2016 International Carnahan Conference on Secu-
rity Technology (ICCST), pp. 1–5, Orlando, FL, USA, October
2016.
[9] S. Haware and A. Barhatte, “Retina based biometric identifica-
tion using SURF and ORB feature descriptors,” in 2017 Inter-
national conference on Microelectronic Devices, Circuits and
Systems (ICMDCS), pp. 1–6, Vellore, India, August 2017.
[10] I. Awate and B. A. Dixit, “Palm print based person identifica-
tion,” in 2015 International Conference on Computing Com-
munication Control and Automation, pp. 781–785, Pune,
India, Febuary 2015.
[11] B. Zhang, W. Li, P. Qing, and D. Zhang, “Palm-Print classifica-
tion by global features,” IEEE Transactions on Systems, Man,
and Cybernetics: Systems, vol. 43, no. 2, pp. 370–378, 2013.
[12] L. Lu, J. Yu, Y. Chen et al., “Lip reading-based user authentica-
tion through acoustic sensing on smartphones,” IEEE/ACM
Transactions on Networking, vol. 27, no. 1, pp. 447–460, 2019.
[13] M. I. Faraj and J. Bigun, “Motion features from lip movement
for person authentication,” in 18th International Conference
on Pattern Recognition (ICPR'06), pp. 1059–1062, Hong Kong,
China, August 2006.
[14] L. V. R. Asuncion, J. X. P. De Mesa, P. K. H. Juan, N. T. Sayson,
and A. R. Dela Cruz, “Thigh motion-based gait analysis for
human
identification
using
inertial
measurement
units
(IMUs),” in 2018 IEEE 10th International Conference on
Humanoid, Nanotechnology, Information Technology, Com-
munication and Control, Environment and Management
(HNICEM), pp. 1–6, Baguio city, Philippines, November-
December 2018.
[15] C. Y. Yam, M. S. Nixon, and J. N. Carter, “Performance anal-
ysis on new biometric gait motion model,” in Proceedings Fifth
IEEE Southwest Symposium on Image Analysis and Interpreta-
tion, pp. 31–34, Sante Fe, NM, USA, April 2002.
[16] D. Yang, X. An, S. Liu, F. He, and D. Ming, “Using convolu-
tional neural networks for identification based on EEG
signals,” in 2018 10th International Conference on Intelligent
Human-Machine
Systems
and
Cybernetics
(IHMSC),
pp. 119–122, Hangzhou, China, August 2018.
[17] G.-Y. Choi, S.-I. Choi, and H.-J. Hwang, “Individual identifica-
tion based on resting-state EEG,” in 2018 6th International
Conference on Brain-Computer Interface (BCI), pp. 1–4, Gang-
Won, South Korea, January 2018.
[18] J. S. Paiva, D. Dias, and J. P. S. Cunha, “Beat-ID: Towards a
computationally low-cost single heartbeat biometric identity
check system based on electrocardiogram wave morphology,”
PLoS One, vol. 12, no. 7, article e0180942, 2017.
[19] X. Dong, W. Si, and W. Huang, “ECG-based identity recogni-
tion via deterministic learning,” Biotechnology & Biotechnolog-
ical Equipment, vol. 32, no. 3, pp. 769–777, 2018.
[20] Y. Y. Gu, Y. Zhang, and Y. T. Zhang, “A novel biometric
approach in human verification by photoplethysmographic
signals,” in 4th International IEEE EMBS Special Topic Confer-
ence on Information Technology Applications in Biomedicine,
2003, pp. 13-14, Birmingham, UK, April 2003.
[21] Y. Y. Gu and Y. T. Zhang, “Photoplethysmographic authenti-
cation through fuzzy logic,” in IEEE EMBS Asian-Pacific Con-
ference on Biomedical Engineering, 2003, pp. 136-137, Kyoto,
Japan, October 2003.
[22] A. B. Hertzman and C. Spielman, “Observations on the finger
volume pulse recorded photoelectrically,” American Journal of
Physiology, vol. 119, pp. 334-335, 1937.
[23] C. Wang, Z. Li, and X. Wei, “Monitoring heart and respiratory
rates at radial artery based on PPG,” Optik, vol. 124, no. 19,
pp. 3954–3956, 2013.
[24] A. R. Kavsaoğlu, K. Polat, and M. R. Bozkurt, “A novel feature
ranking algorithm for biometric recognition with PPG sig-
nals,” Computers in Biology and Medicine, vol. 49, pp. 1–14,
2014.
[25] A. Sarkar, A. Lynn Abbott, and Z. Doerzaph, “Biometric
authentication using photoplethysmography signals,” in 2016
IEEE 8th International Conference on Biometrics Theory,
Applications and Systems (BTAS), Niagara Falls, NY, USA,
September 2016.
[26] S. Koelstra, C. Muhl, M. Soleymani et al., “Deap: a database
for emotion analysis using physiological signals,” IEEE
Transactions on Affective Computing, vol. 3, no. 1, pp. 18–
31, 2012.
[27] V. Jindal, J. Birjandtalab, M. Baran Pouyan, and M. Nourani,
“An adaptive deep learning approach for PPG-based identifi-
cation,” in 2016 38th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBC),
Orlando, FL, USA, August 2016.
[28] Z. Zhang, Z. Pi, and B. Liu, “TROIKA: a general framework for
heart rate monitoring using wrist-type photoplethysmo-
graphic signals during intensive physical exercise,” IEEE
Transactions on Biomedical Engineering, vol. 62, no. 2,
pp. 522–531, 2015.
[29] T. Choudhary and M. S. Manikandan, “Robust photoplethys-
mographic (PPG) based biometric authentication for wireless
body area networks and m-health applications,” in 2016
Twenty Second National Conference on Communication
(NCC), Guwahati, India, March 2016.
[30] J. Luque, G. Cortès, C. Segura, A. Maravilla, J. Esteban, and
J. Fabregat, “End-to-end photopleth ysmography (PPG) based
biometric authentication by using convolutional neural net-
works,” in 2018 26th European Signal Processing Conference
(EUSIPCO), Rome, Italy, September 2018.
[31] U. Yadav, S. N. Abbas, and D. Hatzinakos, “Evaluation of PPG
biometrics for authentication in different states,” in 2018 Inter-
national Conference on Biometrics (ICB), Gold Coast, QLD,
Australia, Febuary 2018.
[32] W. Karlen, S. Raman, J. M. Ansermino, and G. A. Dumont,
“Multiparameter respiratory rate estimation from the photo-
plethysmogram,” IEEE Transactions on Biomedical Engineer-
ing, vol. 60, no. 7, pp. 1946–1953, 2013.
13
Journal of Sensors
[33] D. Biswas, L. Everson, M. Liu et al., “CorNET: deep learning
framework for PPG-based heart rate estimation and biometric
identification in ambulant environment,” IEEE Transactions
on Biomedical Circuits and Systems, vol. 12, no. 2, pp. 282–
291, 2019.
[34] I. Daubechies, Ten Lectures on Wavelets, CBMS-NSF Regional
Conference Series in Applied Mathematics, SIAM Ed, Philadel-
phia, PA, USA, 1992.
[35] S. G. Mallat, “A theory for multiresolution signal decomposi-
tion: the wavelet representation,” IEEE Transactions on Pat-
tern Analysis and Machine Intelligence., vol. 11, no. 7,
pp. 674–693, 1989.
[36] NIST/SEMATECH, e-Handbook of Statistical Methods, 2016,
http://www.itl.nist.gov/div898/handbook/.
[37] M. Borowska, “Entropy-based algorithms in the analysis of
biomedical signals,” Studies in Logic, vol. 43, no. 1, pp. 21–
32, 2015.
[38] L. V. D. Maaten and G. Hinton, “Visualizing data using t-
SNE,”
Journal
of
Machine
Learning
Research,
vol.
9,
pp. 2579–2605, 2008.
[39] H. He and E. A. Garcia, “Learning from imbalanced data,”
IEEE Transactions on Knowledge and Data Engineering,
vol. 21, no. 9, pp. 1263–1284, 2009.
[40] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of
Statistical Learning, second edition, Springer, New York,
NY, USA, 2008.
[41] N. Cristianini and J. Shawe-Taylor, An Introduction to Support
Vector Machines and Other Kernel-Based Learning Methods,
Cambridge University Press, Cambridge, UK, 2000.
[42] J. Han and M. Kamber, Data Mining: Concepts and Techniques
(The Morgan Kaufmann Series in Data Management Systems),
Morgan Kaufmann, San Mateo, CA, USA, 2000.
[43] R. Fisher and A. The, “The use of multiple measurements in
taxonomic problems,” Annals of Eugenics, vol. 7, no. 2,
pp. 179–188, 1936.
[44] L. Breiman, “Random forests,” Machine Learning, vol. 45,
no. 1, pp. 5–32, 2001.
[45] G. H. John and P. Langley, “Estimating continuous distribu-
tions in Bayesian classier,” pp. 338–345, 1995, http://arxiv
.org/abs/1302.4964.
[46] T. M. Cover and P. E. Hart, “Nearest neighbor pattern classifi-
cation,” IEEE Transactions on Information Theory, vol. 13,
no. 1, pp. 21–27, 1967.
[47] D. Chowdhury, M. Sarkar, G. Rabbi, and M. Z. Haider, “A
photoplethysmography based noninvasive cardiac activity
monitoring device offering a customized android application,”
in 2018 10th International Conference on Electrical and Com-
puter Engineering (ICECE), pp. 34–37, Dhaka, Bangladesh,,
December 2018.
14
Journal of Sensors
